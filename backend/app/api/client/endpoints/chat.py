# Copyright 2024 CatWiki Authors
#
# Licensed under the CatWiki Open Source License (Modified Apache 2.0);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://github.com/CatWiki/CatWiki/blob/main/LICENSE
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""èŠå¤©è¡¥å…¨ç«¯ç‚¹ (LangGraph æŒä¹…åŒ–ç‰ˆæœ¬)

åŸºäº LangGraph å®ç°çš„ RAG èŠå¤©æµç¨‹ï¼š
1. ä½¿ç”¨ PostgreSQL Checkpointer æŒä¹…åŒ–ä¼šè¯å†å²
2. ä½¿ç”¨ LangGraph å›¾è¿›è¡Œæ£€ç´¢å’Œæ¶ˆæ¯é¢„å¤„ç†
3. è°ƒç”¨ OpenAI å…¼å®¹ API ç”Ÿæˆå›ç­”
4. æ”¯æŒæµå¼è¾“å‡º
5. è‡ªåŠ¨åˆ›å»º/æ›´æ–° ChatSession è®°å½•
"""

import logging
import uuid
import json
from typing import AsyncGenerator

from fastapi import APIRouter, Header, HTTPException
from fastapi.responses import StreamingResponse
from openai import AsyncOpenAI
from langchain_core.messages import HumanMessage, AIMessage

from app.schemas.chat import (
    ChatCompletionRequest,
    ChatCompletionResponse,
    ChatCompletionChoice,
    ChatCompletionUsage,
    ChatMessage,
    ChatCompletionChunk,
    ChatCompletionChunkChoice,
    ChatCompletionChunkDelta,
)
from app.core.dynamic_config import get_dynamic_chat_config
from app.core.graph import create_agent_graph, langchain_to_openai
from app.core.checkpointer import get_checkpointer
from app.db.database import AsyncSessionLocal
from app.services.chat_session_service import ChatSessionService

router = APIRouter()
logger = logging.getLogger(__name__)


async def stream_generator(
    client: AsyncOpenAI,
    model: str,
    messages: list[dict],
    request: ChatCompletionRequest,
    citations: list = None,
) -> AsyncGenerator[str, None]:
    """æµå¼å“åº”ç”Ÿæˆå™¨"""
    try:
        stream = await client.chat.completions.create(
            model=model,
            messages=messages,
            stream=True,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
            presence_penalty=request.presence_penalty,
            frequency_penalty=request.frequency_penalty,
        )

        full_response = ""
        async for chunk in stream:
            if chunk.choices and chunk.choices[0].delta.content:
                full_response += chunk.choices[0].delta.content
            yield f"data: {chunk.model_dump_json()}\n\n"

        # åœ¨ç»“æŸå‰å‘é€ Citations
        if citations:
            citation_chunk = {"citations": citations}
            yield f"data: {json.dumps(citation_chunk)}\n\n"

        # æ›´æ–°ä¼šè¯æœ€åä¸€æ¡æ¶ˆæ¯
        if full_response:
            async with AsyncSessionLocal() as db:
                await ChatSessionService.update_assistant_response(
                    db=db, thread_id=request.thread_id, assistant_message=full_response
                )

            # åŒæ—¶å°†åŠ©æ‰‹å›å¤å­˜å…¥ LangGraph çŠ¶æ€ï¼Œç¡®ä¿å†å²è®°å½•å®Œæ•´
            async with get_checkpointer() as checkpointer:
                graph = create_agent_graph(checkpointer=checkpointer)
                config = {"configurable": {"thread_id": request.thread_id}}
                await graph.aupdate_state(
                    config, {"messages": [AIMessage(content=full_response)]}, as_node="respond"
                )

        yield "data: [DONE]\n\n"

    except Exception as e:
        logger.error(f"âŒ [Chat] Stream error: {e}")
        error_chunk = ChatCompletionChunk(
            id=f"error-{uuid.uuid4()}",
            model=model,
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(content=f"\n\n[Error: {str(e)}]"),
                    finish_reason="stop",
                )
            ],
        )
        yield f"data: {error_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"


@router.post(
    "/completions", response_model=ChatCompletionResponse, operation_id="createChatCompletion"
)
async def create_chat_completion(
    request: ChatCompletionRequest,
    origin: str | None = Header(None),
    referer: str | None = Header(None),
) -> ChatCompletionResponse | StreamingResponse:
    """
    åˆ›å»ºèŠå¤©è¡¥å…¨ (OpenAI å…¼å®¹æ¥å£) - å…¬å¼€/WebæŒ‚ä»¶ä½¿ç”¨

    å®‰å…¨æœºåˆ¶ï¼š
    1. CORS/Origin æ ¡éªŒï¼šç¡®ä¿è¯·æ±‚æ¥è‡ªå—ä¿¡ä»»çš„åŸŸåï¼ˆé˜²æ­¢è·¨ç«™è°ƒç”¨æ»¥ç”¨ï¼‰
    2. é€Ÿç‡é™åˆ¶ (TODO)ï¼šå»ºè®®åœ¨ç½‘å…³å±‚é…ç½®
    """
    # å¦‚æœæœªæŒ‡å®š site_idï¼Œåˆ™è§†ä¸ºå…¨å±€å¤šç«™ç‚¹æ¨¡å¼ (site_id=0)
    site_id = request.filter.site_id if (request.filter and request.filter.site_id) else 0

    # --- Origin/Referer æ ¡éªŒ ---
    # å…¨å±€ CORS ä¸­é—´ä»¶ (middleware.py) å·²å¤„ç†è·¨åŸŸä¿æŠ¤ã€‚
    # åªæœ‰åœ¨ BACKEND_CORS_ORIGINS å…è®¸åˆ—è¡¨ä¸­çš„åŸŸåæ‰èƒ½é€šè¿‡æµè§ˆå™¨è°ƒç”¨æ­¤æ¥å£ã€‚

    return await _process_chat_request(request, site_id)


@router.post(
    "/site-completions",
    response_model=ChatCompletionResponse,
    operation_id="createSiteChatCompletion",
)
async def create_site_chat_completion(
    request: ChatCompletionRequest,
    authorization: str = Header(..., description="Bearer <api_key>"),
) -> ChatCompletionResponse | StreamingResponse:
    """
    åˆ›å»ºèŠå¤©è¡¥å…¨ (ä¸“ç”¨æ¥å£) - åªæœ‰å¸¦æœ‰æ•ˆ API Key æ‰èƒ½è®¿é—®
    """
    # 1. éªŒè¯ API Key
    if not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Invalid authorization header format")

    token = authorization.replace("Bearer ", "")

    async with AsyncSessionLocal() as db:
        from app.crud.site import crud_site

        site = await crud_site.get_by_api_token(db, api_token=token)

    if not site:
        raise HTTPException(status_code=401, detail="Invalid API Key")

    # 2. å¼ºåˆ¶ä½¿ç”¨ Key å…³è”çš„ Site ID
    site_id = site.id

    # 3. è°ƒç”¨æ ¸å¿ƒé€»è¾‘
    return await _process_chat_request(request, site_id)


async def _process_chat_request(
    request: ChatCompletionRequest, site_id: int
) -> ChatCompletionResponse | StreamingResponse:
    """æ ¸å¿ƒèŠå¤©å¤„ç†é€»è¾‘"""

    # 1. è·å–åŠ¨æ€é…ç½®
    async with AsyncSessionLocal() as db:
        chat_config = await get_dynamic_chat_config(db)

    current_model = chat_config["model"]
    current_api_key = chat_config["apiKey"]
    current_base_url = chat_config["baseUrl"]

    # å®ä¾‹åŒ–å®¢æˆ·ç«¯
    client = AsyncOpenAI(
        api_key=current_api_key,
        base_url=current_base_url,
    )

    # è®°å½•è¯·æ±‚ä¿¡æ¯
    msg_preview = request.message[:200] + "..." if len(request.message) > 200 else request.message

    log_banner = (
        "\n"
        "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  AI Chat Request (LangGraph) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n"
        f"â”‚ ğŸ¤– Model    : {current_model:<50} â”‚\n"
        f"â”‚ ğŸŒŠ Stream   : {str(request.stream):<50} â”‚\n"
        f"â”‚ ğŸ§µ Thread   : {request.thread_id:<50} â”‚\n"
        f"â”‚ ğŸ¢ Site ID  : {site_id:<50} â”‚\n"
        "â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\n"
        f"â”‚ ğŸ—¨ï¸  Message: {msg_preview[:60]:<60} â”‚\n"
        "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯"
    )
    print(log_banner)
    logger.info(
        f"AI Chat Request: model={current_model} thread_id={request.thread_id} site_id={site_id}"
    )

    # 1.5 åˆ›å»º/æ›´æ–°ä¼šè¯è®°å½•
    async with AsyncSessionLocal() as db:
        await ChatSessionService.create_or_update(
            db=db,
            thread_id=request.thread_id,
            site_id=site_id,
            user_message=request.message,
            member_id=request.user,  # ä½¿ç”¨è¯·æ±‚ä¸­çš„ user å­—æ®µä½œä¸º member_id (æ”¯æŒåŒ¿å Visitor ID)
        )

    # 2. ä½¿ç”¨ LangGraph æ‰§è¡Œ RAG æµç¨‹ï¼ˆå¸¦æŒä¹…åŒ–ï¼‰
    try:
        async with get_checkpointer() as checkpointer:
            graph = create_agent_graph(checkpointer=checkpointer)

            # åªä¼ å…¥å½“å‰æ¶ˆæ¯ï¼Œå†å²ç”± Checkpointer è‡ªåŠ¨ç®¡ç†
            initial_state = {
                "messages": [HumanMessage(content=request.message)],
                "context": "",
                "citations": [],
                "should_retrieve": True,
                "should_retrieve": True,
                "rewritten_query": "",
                "site_id": site_id,
            }

            # é…ç½® thread_id
            config = {"configurable": {"thread_id": request.thread_id}}

            logger.info(f"ğŸš€ [Chat] Invoking LangGraph (thread={request.thread_id})...")
            result = await graph.ainvoke(initial_state, config)

        # æå–ç»“æœ
        processed_messages = langchain_to_openai(result["messages"])
        citations = result.get("citations", [])

        logger.info(f"âœ… [Chat] LangGraph completed. Citations: {len(citations)}")

    except Exception as e:
        logger.error(f"âŒ [Chat] LangGraph error: {e}", exc_info=True)
        # é™çº§ï¼šä»…ä½¿ç”¨å½“å‰æ¶ˆæ¯
        processed_messages = [{"role": "user", "content": request.message}]
        citations = []

    # 3. è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
    try:
        if request.stream:
            return StreamingResponse(
                stream_generator(
                    client, current_model, processed_messages, request, citations=citations
                ),
                media_type="text/event-stream",
            )

        # éæµå¼å“åº”
        response = await client.chat.completions.create(
            model=current_model,
            messages=processed_messages,
            stream=False,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
            presence_penalty=request.presence_penalty,
            frequency_penalty=request.frequency_penalty,
        )

        # æ›´æ–°ä¼šè¯æœ€åä¸€æ¡æ¶ˆæ¯
        assistant_message = response.choices[0].message.content or ""
        async with AsyncSessionLocal() as db:
            await ChatSessionService.update_assistant_response(
                db=db, thread_id=request.thread_id, assistant_message=assistant_message
            )

        # åŒæ—¶å°†åŠ©æ‰‹å›å¤å­˜å…¥ LangGraph çŠ¶æ€ï¼Œç¡®ä¿å†å²è®°å½•å®Œæ•´
        async with get_checkpointer() as checkpointer:
            graph = create_agent_graph(checkpointer=checkpointer)
            config = {"configurable": {"thread_id": request.thread_id}}
            await graph.aupdate_state(
                config, {"messages": [AIMessage(content=assistant_message)]}, as_node="respond"
            )

        return ChatCompletionResponse(
            id=response.id,
            object=response.object,
            created=response.created,
            model=response.model,
            choices=[
                ChatCompletionChoice(
                    index=c.index,
                    message=ChatMessage(role=c.message.role, content=c.message.content or ""),
                    finish_reason=c.finish_reason,
                )
                for c in response.choices
            ],
            usage=ChatCompletionUsage(
                prompt_tokens=response.usage.prompt_tokens,
                completion_tokens=response.usage.completion_tokens,
                total_tokens=response.usage.total_tokens,
            )
            if response.usage
            else None,
        )

    except Exception as e:
        logger.error(f"âŒ [Chat] API Error: {e}", exc_info=True)
        raise e
