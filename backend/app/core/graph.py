
"""LangGraph ReAct Agent
1. ReAct å¾ªçŽ¯: Agent -> Tools -> Agent ... -> End
2. æ”¯æŒå¤šè½®æ£€ç´¢å’ŒæŽ¨ç†
3. åŠ¨æ€å¼•ç”¨æå–
4. è‡ªåŠ¨å¯¹è¯æ‘˜è¦ (é•¿æœŸè®°å¿†)
"""

import logging
import json
from typing import Literal, List, Annotated

from langchain_core.messages import SystemMessage, BaseMessage, ToolMessage, HumanMessage, RemoveMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langchain_core.runnables import RunnableConfig

from app.schemas.graph_state import ChatGraphState
from app.services.vector_service import VectorService
from app.schemas.document import VectorRetrieveFilter
from app.core.prompts import SYSTEM_PROMPT, NO_RESULTS_MESSAGE, SUMMARIZE_PROMPT

logger = logging.getLogger(__name__)

# æœ€å¤§è¿­ä»£æ¬¡æ•°é™åˆ¶ï¼Œé˜²æ­¢ Agent æ— é™å¾ªçŽ¯ï¼ˆä»Žé…ç½®è¯»å–ï¼‰
from app.core.config import settings

MAX_ITERATIONS = settings.AGENT_MAX_ITERATIONS


# =============================================================================
# å·¥å…·å®šä¹‰
# =============================================================================


@tool
async def search_knowledge_base(query: str, config: RunnableConfig) -> str:
    """åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯ã€‚

    å½“ç”¨æˆ·çš„é—®é¢˜éœ€è¦äº‹å®žä¾æ®ã€æ–‡æ¡£æ”¯æŒæˆ–ä½ ä¸çŸ¥é“ç­”æ¡ˆæ—¶ï¼Œ**å¿…é¡»**ä½¿ç”¨æ­¤å·¥å…·ã€‚
    å¯ä»¥å¤šæ¬¡è°ƒç”¨æ­¤å·¥å…·ä»¥æŸ¥æ‰¾ä¸åŒæ–¹é¢çš„ä¿¡æ¯ã€‚

    Args:
        query: æœç´¢æŸ¥è¯¢è¯ã€‚åº”è¯¥æ˜¯é’ˆå¯¹ç‰¹å®šä¿¡æ¯çš„æ¸…æ™°é—®é¢˜ã€‚

    Returns:
        JSON æ ¼å¼çš„å­—ç¬¦ä¸²ï¼ŒåŒ…å«æœç´¢ç»“æžœåˆ—è¡¨ã€‚
        æ¯ä¸ªç»“æžœåŒ…å« 'content' (å†…å®¹æ‘˜å½•) å’Œ 'metadata' (åŒ…å« title, document_id ç­‰)ã€‚
    """
    # èŽ·å–ç«™ç‚¹ä¸Šä¸‹æ–‡
    site_id = config.get("configurable", {}).get("site_id")
    logger.info(f"ðŸ”§ [Tool] search_knowledge_base: query='{query}', site_id={site_id}")

    try:
        search_filter = VectorRetrieveFilter(site_id=int(site_id)) if site_id else None

        # æ‰§è¡Œæ£€ç´¢
        retrieved_docs = await VectorService.retrieve(
            query=query,
            k=5,
            threshold=0.3,
            filter=search_filter,
        )

        if not retrieved_docs:
            return NO_RESULTS_MESSAGE

        # æ ¼å¼åŒ–ç»“æžœ
        results = [
            {
                "content": doc.content,
                "metadata": {
                    "document_id": doc.document_id,
                    "title": doc.document_title,
                    "score": doc.score,
                    **doc.metadata,
                },
            }
            for doc in retrieved_docs
        ]

        return json.dumps(results, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ [Tool] Knowledge base search failed: {e}", exc_info=True)
        return f"æœç´¢çŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}"


# å·¥å…·åˆ—è¡¨
tools = [search_knowledge_base]


# =============================================================================
# è¾…åŠ©å‡½æ•°ï¼šå¼•ç”¨æå–
# =============================================================================


def extract_citations_from_messages(messages: List[BaseMessage], from_last_turn: bool = False) -> List[dict]:
    """ä»ŽåŽ†å²æ¶ˆæ¯çš„ ToolMessage ä¸­æå–å¼•ç”¨

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        from_last_turn: æ˜¯å¦ä»…æå–æœ€åŽä¸€è½®å¯¹è¯çš„å¼•ç”¨ (ä»Žæœ€åŽä¸€æ¡ HumanMessage å¼€å§‹)
    """
    citations = {}
    target_messages = messages

    if from_last_turn:
        # æ‰¾åˆ°æœ€åŽä¸€æ¡ HumanMessage çš„ç´¢å¼•
        last_human_idx = -1
        for i in range(len(messages) - 1, -1, -1):
            if isinstance(messages[i], HumanMessage):
                last_human_idx = i
                break
        
        if last_human_idx != -1:
            target_messages = messages[last_human_idx:]

    for msg in target_messages:
        if isinstance(msg, ToolMessage) and msg.name == "search_knowledge_base":
            try:
                content = msg.content if isinstance(msg.content, str) else json.dumps(msg.content)
                results = json.loads(content)
                
                if isinstance(results, list):
                    for doc in results:
                        meta = doc.get("metadata", {})
                        doc_id = meta.get("document_id")
                        if doc_id and doc_id not in citations:
                            citations[doc_id] = {
                                "id": str(doc_id),
                                "title": meta.get("title", "Unknown"),
                                "siteId": meta.get("site_id"),
                                "documentId": doc_id,
                                "score": meta.get("score"),
                            }
            except (json.JSONDecodeError, AttributeError):
                continue
            except Exception as e:
                logger.error(f"âŒ Error extracting citations: {e}")

    return list(citations.values())


# =============================================================================
# Agent å›¾æž„å»º
# =============================================================================


def create_agent_graph(checkpointer=None, model: ChatOpenAI = None):
    """åˆ›å»º ReAct Agent å›¾

    Args:
        checkpointer: å¯é€‰çš„ Checkpointer å®žä¾‹
        model: é…ç½®å¥½çš„ LLM å®žä¾‹ (å¿…é¡»æ”¯æŒ bind_tools)

    Returns:
        ç¼–è¯‘åŽçš„ StateGraph
    """
    if model is None:
        raise ValueError("Model must be provided to create_agent_graph")

    # 1. ç»‘å®šå·¥å…·åˆ°æ¨¡åž‹
    model_with_tools = model.bind_tools(tools)

    # 2. å®šä¹‰èŠ‚ç‚¹
    async def agent_node(state: ChatGraphState) -> dict:
        """Agent å†³ç­–èŠ‚ç‚¹"""
        logger.debug("ðŸ¤– [Agent] Thinking...")
        messages = state["messages"]

        # æ³¨å…¥ System Prompt å’Œ æ‘˜è¦
        system_content = SYSTEM_PROMPT
        if state.get("summary"):
           system_content += f"\n\n#### ä¹‹å‰çš„å¯¹è¯æ‘˜è¦ ####\n{state['summary']}"

        if not messages or not isinstance(messages[0], SystemMessage):
            messages = [SystemMessage(content=system_content)] + list(messages)
        else:
             # å¦‚æžœå·²æœ‰ SystemMessage (ä¾‹å¦‚æŒä¹…åŒ–ä¸‹æ¥çš„)ï¼Œæ›´æ–°å…¶å†…å®¹
             # æ³¨æ„ï¼šæ¯æ¬¡è°ƒç”¨ agent_node éƒ½æ›´æ–° System Prompt æ˜¯ä¸ªå¥½åšæ³•ï¼Œç¡®ä¿æ‘˜è¦æœ€æ–°
             messages[0] = SystemMessage(content=system_content)

        response = await model_with_tools.ainvoke(messages)
        return {"messages": [response]}

    async def summarize_conversation(state: ChatGraphState) -> dict:
        """å¯¹è¯æ‘˜è¦èŠ‚ç‚¹"""
        logger.info("ðŸ“ [Summarize] Summarizing conversation history...")
        messages = state["messages"]
        summary = state.get("summary", "")

        # æž„é€ æ‘˜è¦ prompt
        summarize_message = SUMMARIZE_PROMPT
        if summary:
            summarize_message += f"\n\n(çŽ°æœ‰æ‘˜è¦: {summary})"
        
        # åªå–é™¤äº† SystemMessage ä¹‹å¤–çš„æ¶ˆæ¯è¿›è¡Œæ‘˜è¦
        conversation_messages = [msg for msg in messages if not isinstance(msg, SystemMessage)]
        
        # å¦‚æžœæ²¡æœ‰è¶³å¤Ÿçš„æ¶ˆæ¯éœ€è¦æ‘˜è¦ï¼ˆè™½ç„¶è·¯ç”±é€»è¾‘åº”è¯¥å·²ç»è¿‡æ»¤äº†ï¼Œä½†åšä¸ªé˜²å¾¡ï¼‰
        if not conversation_messages:
            return {}

        # æ·»åŠ æ‘˜è¦æŒ‡ä»¤ (HumanMessage)
        prompt_messages = conversation_messages + [HumanMessage(content=summarize_message)]

        # è°ƒç”¨æ¨¡åž‹ç”Ÿæˆæ‘˜è¦ (ä½¿ç”¨æœªç»‘å®šå·¥å…·çš„åŸºç¡€æ¨¡åž‹ï¼Œæˆ–è€…åŒä¸€ä¸ªæ¨¡åž‹ä½† ignore tools)
        # è¿™é‡Œç›´æŽ¥ç”¨ model (æœª bind_tools) å¯èƒ½ä¼šæ›´çº¯ç²¹ï¼Œä½† create_agent_graph å‚æ•°ä¼ è¿›æ¥çš„æ˜¯ model è¿˜æ˜¯ model_with_tools?
        # å‚æ•°æ˜¯ `model: ChatOpenAI` (åŽŸå§‹æ¨¡åž‹)ã€‚
        response = await model.ainvoke(prompt_messages)
        new_summary = response.content
        logger.info(f"ðŸ“ [Summarize] New summary: {new_summary[:100]}...")

        # åˆ é™¤æ—§æ¶ˆæ¯ï¼Œä¿ç•™æœ€è¿‘çš„ N æ¡äº¤äº’
        # ç­–ç•¥ï¼šä¿ç•™æœ€åŽ 6 æ¡æ¶ˆæ¯ (é€šå¸¸æ˜¯ H-A-T-A-H-A)
        KEEP_LAST_N = 6
        delete_messages = []
        if len(conversation_messages) > KEEP_LAST_N:
             # è¦åˆ é™¤çš„æ¶ˆæ¯ ID
             # conversation_messages[:-6] æ˜¯é™¤äº†æœ€åŽ 6 æ¡ä¹‹å¤–çš„æ‰€æœ‰æ¶ˆæ¯
             messages_to_delete = conversation_messages[:-KEEP_LAST_N]
             delete_messages = [RemoveMessage(id=m.id) for m in messages_to_delete]
             logger.info(f"ðŸ—‘ï¸ [Summarize] Pruning {len(delete_messages)} old messages")

        return {"summary": new_summary, "messages": delete_messages}

    # 3. æž„å»ºå›¾
    graph_builder = StateGraph(ChatGraphState)

    # å·¥å…·èŠ‚ç‚¹åŒ…è£…å™¨ï¼šé€’å¢žè¿­ä»£è®¡æ•° + æ£€æµ‹ç©ºç»“æžœ
    tool_node = ToolNode(tools)

    # è¿žç»­ç©ºç»“æžœç»ˆæ­¢é˜ˆå€¼ï¼ˆä»Žé…ç½®è¯»å–ï¼‰
    MAX_CONSECUTIVE_EMPTY = settings.AGENT_MAX_CONSECUTIVE_EMPTY
    SUMMARY_TRIGGER_COUNT = settings.AGENT_SUMMARY_TRIGGER_MSG_COUNT

    async def tools_wrapper_node(state: ChatGraphState) -> dict:
        """å·¥å…·èŠ‚ç‚¹åŒ…è£…å™¨ï¼Œæ‰§è¡Œå·¥å…·å¹¶è¿½è¸ªè¿­ä»£è®¡æ•°å’Œç©ºç»“æžœ"""
        # è°ƒç”¨åŽŸå§‹å·¥å…·èŠ‚ç‚¹
        result = await tool_node.ainvoke(state)

        # é€’å¢žè¿­ä»£è®¡æ•°
        current_count = state.get("iteration_count", 0)
        result["iteration_count"] = current_count + 1

        # æ£€æµ‹å·¥å…·è¿”å›žæ˜¯å¦ä¸ºç©ºç»“æžœ
        consecutive_empty = state.get("consecutive_empty_count", 0)
        is_empty_result = False

        # æ£€æŸ¥æœ€åŽä¸€æ¡å·¥å…·æ¶ˆæ¯æ˜¯å¦ä¸ºç©ºç»“æžœ
        if result.get("messages"):
            last_tool_msg = result["messages"][-1] if result["messages"] else None
            if last_tool_msg:
                content = getattr(last_tool_msg, "content", "")
                # æ£€æµ‹ç©ºç»“æžœæ ‡å¿—
                if content == NO_RESULTS_MESSAGE or "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£" in content or content == "[]":
                    is_empty_result = True

        if is_empty_result:
            result["consecutive_empty_count"] = consecutive_empty + 1
            logger.debug(
                f"ðŸ”„ [Graph] Empty result, consecutive count: {result['consecutive_empty_count']}/{MAX_CONSECUTIVE_EMPTY}"
            )
        else:
            result["consecutive_empty_count"] = 0  # é‡ç½®

        logger.debug(f"ðŸ”„ [Graph] Iteration count: {result['iteration_count']}/{MAX_ITERATIONS}")
        return result

    # æ¡ä»¶è·¯ç”±å‡½æ•°ï¼šæ£€æŸ¥è¿­ä»£æ¬¡æ•°é™åˆ¶ + è¿žç»­ç©ºç»“æžœ
    def route_after_agent(state: ChatGraphState) -> Literal["tools", "should_summarize"]:
        """Agent åŽçš„è·¯ç”±å†³ç­–"""
        messages = state["messages"]
        last_message = messages[-1] if messages else None

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        if last_message and hasattr(last_message, "tool_calls") and last_message.tool_calls:
            # æ£€æŸ¥è¿­ä»£æ¬¡æ•°
            current_count = state.get("iteration_count", 0)
            if current_count >= MAX_ITERATIONS:
                logger.warning(f"âš ï¸ [Graph] Max iterations ({MAX_ITERATIONS}) reached, stopping tools")
                return "should_summarize"

            # æ£€æŸ¥è¿žç»­ç©ºç»“æžœ
            consecutive_empty = state.get("consecutive_empty_count", 0)
            if consecutive_empty >= MAX_CONSECUTIVE_EMPTY:
                logger.warning(
                    f"âš ï¸ [Graph] {MAX_CONSECUTIVE_EMPTY} consecutive empty results, stopping tools"
                )
                return "should_summarize"

            return "tools"

        return "should_summarize"
    


    async def check_summary_node(state: ChatGraphState) -> dict:
        """æ£€æŸ¥æ‘˜è¦èŠ‚ç‚¹çš„å ä½ç¬¦ï¼ˆPass-through nodeï¼‰"""
        # è¯¥èŠ‚ç‚¹ä¸ä¿®æ”¹çŠ¶æ€ï¼Œä»…ä½œä¸ºæ¡ä»¶è·¯ç”±çš„ä¸­è½¬
        return {}

    def should_summarize(state: ChatGraphState) -> Literal["summarize_conversation", "__end__"]:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ‘˜è¦"""
        messages = state["messages"]
        
        # ç®€å•ç­–ç•¥ï¼šéž System æ¶ˆæ¯æ€»æ•°è¶…è¿‡é˜ˆå€¼åˆ™è§¦å‘æ‘˜è¦
        non_system_msgs = [m for m in messages if not isinstance(m, SystemMessage)]
        
        # å®žé™…ç”Ÿäº§ä¸­å¯ä»¥è®¡ç®— Token æ•°
        if len(non_system_msgs) > SUMMARY_TRIGGER_COUNT:
             logger.info(f"ðŸ“Š [Graph] Message count {len(non_system_msgs)} > {SUMMARY_TRIGGER_COUNT}, triggering summarization")
             return "summarize_conversation"
        
        return "__end__"

    graph_builder.add_node("agent", agent_node)
    graph_builder.add_node("tools", tools_wrapper_node)
    graph_builder.add_node("summarize_conversation", summarize_conversation)
    graph_builder.add_node("check_summary_node", check_summary_node)

    # 4. å®šä¹‰è¾¹
    graph_builder.add_edge(START, "agent")

    # æ¡ä»¶è¾¹: Agent -> (Tools | Check Summary)
    graph_builder.add_conditional_edges(
        "agent",
        route_after_agent,
        {
            "tools": "tools",
            "should_summarize": "check_summary_node"
        }
    )

    # å¾ªçŽ¯è¾¹: Tools -> Agent
    graph_builder.add_edge("tools", "agent")

    # æ¡ä»¶è¾¹: Check Summary -> (Summarize | End)
    graph_builder.add_conditional_edges(
        "check_summary_node",
        should_summarize,
        {
            "summarize_conversation": "summarize_conversation",
            "__end__": END
        }
    )

    # æ‘˜è¦ç»“æŸåŽ -> End
    graph_builder.add_edge("summarize_conversation", END)

    return graph_builder.compile(checkpointer=checkpointer)
