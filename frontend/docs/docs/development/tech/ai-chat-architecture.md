# AI 对话系统架构与 RAG 逻辑文档

本文旨在详细介绍 CatWiki AI 对话系统的核心架构、处理流程以及针对 RAG（检索增强生成）质量与效率所做的深度优化。

## 1. 核心架构：基于 LangGraph 的 ReAct 模式

对话系统采用了 **ReAct (Reasoning and Acting)** 范式，利用 [LangGraph](https://langchain-ai.github.io/langgraph/) 构建了一个有状态的循环工作流。

### 1.1 工作流节点 (Nodes)
- **Agent 节点**: 负责思考。LLM 根据历史消息和当前上下文决定是直接回答用户，还是调用工具（如搜索知识库）。
- **Tools 节点**: 负责执行。当 Agent 生成工具调用指令时，系统自动切换到此节点运行特定工具。
- **Summarize 节点**: 负责长期记忆管理。当对话条数达到上限时，自动触发摘要生成并清理历史消息，通过 `RemoveMessage` 机制优化 Token 消耗。

### 1.2 循环逻辑 (Edges)
1. **START -> Agent**: 开始处理用户输入。
2. **Agent -> Tools**: 如果 LLM 决定搜索知识库。
3. **Tools -> Agent**: 将搜索结果反馈给 LLM，重新进入思考环节。
4. **Agent -> END**: LLM 认为已获取足够信息并给出了最终回答。
5. **Agent -> Summarize**: 如果满足摘要触发条件，则流向摘要生成逻辑。

---

## 2. RAG 检索深度优化

为了解决复杂查询（如医学流程、规范步骤）中的信息碎片化和召回不全问题，系统实施了以下核心优化：

### 2.1 语义分片检索 (Chunk Retrieval)
- **策略**: 文档被切分为约 500 字的分片。系统通过向量相似度直接召回最相关的分片。
- **重排序**: 召回后，系统会利用 Reranker 对候选分片进行二次精排，确保最准确的信息排在最前供 AI 使用。

### 2.2 动态配置缓存 (DynamicConfigManager)
- **挑战**: Embedding、Reranker 和 Chat 模型配置存储在数据库中，每轮对话重复查询会大幅增加延迟。
- **方案**: 实现了一个基于单例模式的全局管理器，对 AI 配置进行 5 分钟 TTL 的时间缓存，显著降低了后端接口响应时间（TTFT）。

---

## 3. 引用来源精准映射 (Citation Strategy)

系统采用了一套严密的机制，确保回答中标记的 `[n]` 与底部的“引用来源”列表 100% 对应。

### 3.1 全局会话索引 (Global Indexing)
- 每一条搜索结果都被分配一个在**当前会话会话中唯一且递增**的编号（如第 1 轮搜到 1-10，第 2 轮搜到 11-20）。
- 系统通过指令严格禁止 AI 重新编号，必须以此绝对序号进行引用。

### 3.2 Title-Sync 标题对齐技术
- **逻辑**: 后端提取程序会同时解析 AI 回复中的序号 `[n]` 和它提到的具体标题（如《心肺复苏操作指南》）。
- **对比**: 即使 AI 偶尔数错序号，系统也会通过标题在文档池中进行模糊匹配，强制修正来源展示。
- **精准过滤**: 只有在正文中被真正标记到的文档才会出现在底部的“引用来源”列表中。

### 3.3 内文本引用处理 (Index-Link)
- **挑战**: AI 回复正文中的 `[n]` 标识与检索池文档的一致性。
- **方案**: 系统采用最基础的索引匹配。通过识别回复正文中的 `[n]` 标记，直接对应检索回来的文档池序号。不进行标题二次校验，保持逻辑的极简与透明。

---
